{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAPA: Language Pattern Analyser\n",
    "\n",
    "A Digital Tool for the Analysis of Patterns in Spelled Language Sounds in Historical Dutch Theatre Plays.\n",
    "\n",
    "LAPA allows for converting digitised early modern Dutch theatre plays into (presumed) phonetic script (SAMPA). To achieve this, a ruleset has been created that codifies the transliteration to SAMPA. This codebase contains parsers for the rule sets (xls format), parsers for the digitised texts (naf xml) and logic to perform counts and correlations.\n",
    "\n",
    "This notebook is just a quick placeholder showing how to run a quick analysis of a text. Mode examples will be added shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from lapa_ng.factory import create_matcher\n",
    "from lapa_ng.naf import parse_naf\n",
    "from lapa_ng.text_clean import clean_words, default_cleaners\n",
    "from lapa_ng.translator import CachedTranslator, MatchingTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matcher = create_matcher(\"ng:../fixtures/RULES_A_V1.5.xls#RULES\")\n",
    "translator = MatchingTranslator(matcher)\n",
    "translator = CachedTranslator(translator)\n",
    "  \n",
    "print(f\"Loaded {matcher} matcher.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor pipeline to clean the text prior to tokenization\n",
    "from collections import Counter\n",
    "\n",
    "input = parse_naf(\"../fixtures/vond001gysb04_01.xml\")\n",
    "input = list(clean_words(input, default_cleaners))\n",
    "translations = list(translator.translate(input, emit=\"word\"))\n",
    "\n",
    "print(f\"Created {len(translations)} translations.\")\n",
    "\n",
    "# Count the number of times each word appears in the text\n",
    "unique_translations = Counter((t.word.text.lower(), \" \".join([p.sampa for p in t.phonemes])) for t in translations)\n",
    "print(f\"Found {len(unique_translations)} unique word translations in the text.\")\n",
    "\n",
    "# Most common words\n",
    "most_common_words = unique_translations.most_common()\n",
    "print(f\"The most common words are: {most_common_words[:10]}\")\n",
    "\n",
    "# Least common words\n",
    "print(f\"The least common words are: {most_common_words[-10:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we looked at some basic statistics of the text. One slightly odd thing about the translate function is that it emits a result for each rule match in the document. But a rule can emit zero or more phonemes.\n",
    "\n",
    "I don't think we even really want to work with rule matches, apart from for tracing purposes, so we have the functions `coalesce_translations` and `explode_translations` to convert the list of rule matches into a list of words and a list of phonemes respectively. Below we see how we can use it to extract all phonemes across the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_translations = list(translator.translate(input, emit=\"phoneme\"))\n",
    "\n",
    "# We can now count the number of times each phoneme appears in the text\n",
    "phoneme_counts = Counter(t.phoneme_str() for t in phoneme_translations)\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "df = pd.DataFrame({\n",
    "    'Phoneme': list(phoneme_counts.keys()),\n",
    "    'Count': list(phoneme_counts.values())\n",
    "})\n",
    "\n",
    "# Sort by count\n",
    "df = df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Create pie chart\n",
    "fig = px.pie(df, \n",
    "             values='Count', \n",
    "             names='Phoneme',\n",
    "             title='Distribution of Phonemes in the Text',\n",
    "             hover_data=['Count'],\n",
    "             labels={'Count': 'Frequency'})\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation='v',\n",
    "        yanchor='auto',\n",
    "        y=1,\n",
    "        xanchor='left',\n",
    "        x=1.05\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be a bit easier to read as a  bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total count for percentage calculation\n",
    "total_count = df['Count'].sum()\n",
    "\n",
    "# Create horizontal bar chart with custom hover template\n",
    "fig = px.bar(df, \n",
    "             x='Count', \n",
    "             y='Phoneme',\n",
    "             title='Distribution of Phonemes in the Text',\n",
    "             orientation='h',\n",
    "             labels={'Count': 'Frequency'})\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    height=800,  # Adjust height to fit all phonemes\n",
    "    margin=dict(l=100, r=20, t=50, b=50)  # Adjust margins\n",
    ")\n",
    "\n",
    "# Sort bars by count\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "\n",
    "# Custom hover template showing both count and percentage\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<b>%{y}</b><br>\" +\n",
    "                 \"Count: %{x}<br>\" +\n",
    "                 \"Percentage: %{customdata:.1f}%<br>\" +\n",
    "                 \"<extra></extra>\",\n",
    "    customdata=df['Count'] / total_count * 100\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the word and phoneme counts to see how different words contribute to the total phoneme counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create an object that counts phonemes in each word by looping over each word and splitting the phonemes into a list\n",
    "all_words = {}\n",
    "for word, phonemes in unique_translations:\n",
    "    all_words[word] = Counter(phonemes.split(\" \"))\n",
    "\n",
    "df = pd.DataFrame.from_dict(all_words, orient='index').fillna(0).astype(int)\n",
    "\n",
    "df_phoneme_counts = df.copy()\n",
    "df_phoneme_counts[\"Total\"] = df_phoneme_counts.sum(axis=1)\n",
    "df_phoneme_counts.loc[\"Total\"] = df_phoneme_counts.sum(axis=0)\n",
    "df_phoneme_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the real contribution obviously comes from the words that are used most often, so let's multiple the whole matrix by the word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {k[0]: v for k, v in unique_translations.items()}\n",
    "df_word_frequencies = pd.DataFrame.from_dict(word_frequencies, orient='index', columns=[\"Count\"])\n",
    "df_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weighted = df.mul(df_word_frequencies['Count'], axis=0).fillna(0).astype(int)\n",
    "df_weighted\n",
    "\n",
    "df_weighted_sums = df_weighted.copy()\n",
    "df_weighted_sums.loc[\"Total\"] = df_weighted_sums.sum(axis=0)\n",
    "df_weighted_sums\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chart is a bit insane, so don't worry if it takes a long time to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "other_label = \"Other\"\n",
    "\n",
    "# 1. Collect top words per phoneme (column)\n",
    "top_words_all = set()\n",
    "top_words_per_phoneme = {}\n",
    "\n",
    "for phoneme in df_weighted.columns:\n",
    "    word_contributions = df_weighted[phoneme] \n",
    "    n_largest = word_contributions.nlargest(top_n).to_dict()\n",
    "    others = word_contributions.sum() - sum(n_largest.values())\n",
    "    n_largest['others'] = others\n",
    "    \n",
    "    top_words_per_phoneme[phoneme] =n_largest\n",
    "\n",
    "\n",
    "df_stacked = pd.DataFrame(top_words_per_phoneme).fillna(0).astype(int).T\n",
    "columns = list(df_stacked.columns)\n",
    "\n",
    "# Move others to the front\n",
    "columns.remove('others')\n",
    "columns.sort()\n",
    "columns.insert(0, 'others')\n",
    "\n",
    "traces = []\n",
    "for word in columns:\n",
    "    # Compute percentage contribution of this word for each phoneme\n",
    "    percentages = df_stacked[word] / df_stacked.sum(axis=1) * 100\n",
    "\n",
    "    traces.append(go.Bar(\n",
    "        y=df_stacked.index,  # phonemes\n",
    "        x=df_stacked[word],  # word's contribution\n",
    "        name=word,\n",
    "        orientation='h',\n",
    "        customdata=percentages,\n",
    "        hovertemplate=(\n",
    "            \"<b>Phoneme: %{y}</b><br>\" +\n",
    "            \"Word: \" + word + \"<br>\" +\n",
    "            \"Count: %{x}<br>\" +\n",
    "            \"Contribution: %{customdata:.1f}%<br>\" +\n",
    "            \"<extra></extra>\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "fig = go.Figure(data=traces)\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Stacked Contribution of Words to Phonemes',\n",
    "    xaxis_title='Total Weighted Phoneme Count',\n",
    "    yaxis_title='Phoneme',\n",
    "    height=800,\n",
    "    margin=dict(l=120, r=40, t=50, b=50),\n",
    "    legend_title_text='Word'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another intersting diagnostic may be to group words that sound the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sounds = defaultdict(list)\n",
    "for word, sound in unique_translations.keys():\n",
    "    sounds[sound].append(word)\n",
    "\n",
    "# Multisounds (filter out those with just one word)\n",
    "multisounds = [dict(phonemes=phonemes, words=words) for phonemes, words in sounds.items() if len(words) > 1]\n",
    "multisounds = sorted(multisounds, key=lambda x: len(x['words']), reverse=True)\n",
    "\n",
    "multisounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapa-ng-jvFzo7la-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
